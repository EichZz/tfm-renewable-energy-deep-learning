{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python --version\n",
    "# %pip install --upgrade pandas\n",
    "# %pip install openpyxl\n",
    "# %pip install numpy\n",
    "# %pip install scikit-learn\n",
    "# %pip install matplotlib\n",
    "# %pip install plot_keras_history\n",
    "# %pip uninstall tensorflow\n",
    "# %pip install keras-tuner==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "        print(\"Setting GPU Memory Growth...\")\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        \n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from plot_keras_history import plot_history\n",
    "import sklearn.metrics as metrics\n",
    "import os as os\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import kerastuner as kt\n",
    "from kerastuner.tuners import RandomSearch, BayesianOptimization\n",
    "\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"KT version:\", kt.__version__)\n",
    "print(\"Panda version:\", pd.__version__)\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(\"Physical devices: \", tf.config.list_physical_devices())\n",
    "# Se va a habilitar la dedicacion dinamica de memoria para que la GPU vaya asignando recursos al proceso conforme los vaya necesitando\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "#print(tf.config.list_physical_devices('GPU'))\n",
    "print(\"Build with CUDA: \", tf.test.is_built_with_cuda())\n",
    "\n",
    "# Number of measurements to predict\n",
    "numPredictions = 20\n",
    "\n",
    "# Number of best models\n",
    "numBestModels = 1 \n",
    "\n",
    "# Testing set percentage\n",
    "test_size = 0.3\n",
    "# Validation set percentage\n",
    "val_size = 0.3\n",
    "\n",
    "\n",
    "# Batch size\n",
    "batch = 1024\n",
    "\n",
    "# Nodos internos\n",
    "hidden_nodes = 10\n",
    "\n",
    "# Units parameters\n",
    "minUnits = 50\n",
    "maxUnits = 200\n",
    "stepsUnits = 50\n",
    "defaultUnits = 50\n",
    "\n",
    "# Layers parameters\n",
    "minLayers = 1\n",
    "maxLayers = 10\n",
    "defaultLayers = 3\n",
    "\n",
    "# Dropout Parameters\n",
    "minDropout = 0\n",
    "maxDropout = 0.33\n",
    "defaultDropout = .25\n",
    "\n",
    "# Establecer medida de loss\n",
    "# loss = \"mean_squared_error\"\n",
    "loss = \"mean_absolute_error\"\n",
    "# loss = \"mean_absolute_percentage_error\"\n",
    "\n",
    "# Learning rate\n",
    "learningRate = [0.0, 1e-2, 1e-3, 1e-4]\n",
    "\n",
    "# Optimizer objetive: error percetange with the validation set \n",
    "objective = kt.Objective('val_mean_absolute_error', 'min')\n",
    "\n",
    "# Maximum model trials and executions\n",
    "trials = 5\n",
    "executions = 5\n",
    "\n",
    "# Model saving parameters\n",
    "workingDirectory = datetime.datetime.fromtimestamp(time.time()).strftime('%d-%m-%Y-%H:%M')\n",
    "# projectName = \"lstm\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs\n",
    "epchs = 1000\n",
    "projectName = \"lstm30min2016-1kepochs-rs\"\n",
    "# projectName = \"lstm30min2016-1kepochs-bo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"C:/Users/hecto/Documents/Master/TFM/tfm-renewable-energy-deep-learning/data/2016_30min.xlsx\",\n",
    "                    header = None, \n",
    "                    engine = 'openpyxl')[0]\n",
    "\n",
    "df2 = pd.read_excel(\"C:/Users/hecto/Documents/Master/TFM/tfm-renewable-energy-deep-learning/data/2016_30min.xlsx\",\n",
    "                    header = None, \n",
    "                    engine = 'openpyxl')[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing: each row will contain the 20 measures for each day , and the 10 measures for the following day\n",
    "X = pd.DataFrame(np.array(df).reshape(-1, 20))\n",
    "Y = pd.DataFrame.copy(X)\n",
    "\n",
    "Y.columns = [\"col_{}\".format(i) for i in range(21, 41)]\n",
    "Y = Y.drop(0)\n",
    "Y = Y.reset_index(drop = True)\n",
    "Y.loc[len(Y)] = np.zeros(numPredictions)\n",
    "\n",
    "# Last row is deleted because it is the one used for the real prediction, \n",
    "# it is not useful for the training of the model. \n",
    "X.drop(X.tail(1).index, inplace = True)\n",
    "Y.drop(Y.tail(1).index, inplace = True)\n",
    "\n",
    "print(\"X Preproccessed shape: \", X.shape)\n",
    "print(\"Y Preproccessed shape: \", Y.shape)\n",
    "print(\"---------------------------------------------\")\n",
    "\n",
    "#  Uncomment in order to normalize data\n",
    "# Data Normalization\n",
    "# scaler = MinMaxScaler()\n",
    "# X = pd.DataFrame(scaler.fit_transform(X), columns=[\"col_{}\".format(i) for i in range(1, 21)])\n",
    "# Y = pd.DataFrame(scaler.fit_transform(Y), columns=[\"col_{}\".format(i) for i in range(21, 41)])\n",
    "\n",
    "dfPreproccessed = pd.concat([X, Y], axis = 1)\n",
    "\n",
    "print(\"DataFrame Preproccessed:\")\n",
    "print(dfPreproccessed)\n",
    "print(\"---------------------------------------------\")\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X, \n",
    "                                                Y, \n",
    "                                                test_size = test_size, \n",
    "                                                random_state = 0, \n",
    "                                                shuffle=False)\n",
    "xTrain, xVal, yTrain, yVal = train_test_split(xTrain, \n",
    "                                            yTrain, \n",
    "                                            test_size = val_size, \n",
    "                                            random_state = 0,\n",
    "                                            shuffle=False)\n",
    "\n",
    "# Adding the third dimension needed to use LSTM (samples, timestamps, features)\n",
    "xtrain = np.reshape(xTrain.values, (xTrain.shape[0], xTrain.shape[1], 1))\n",
    "ytrain = np.reshape(yTrain.values, (yTrain.shape[0], yTrain.shape[1], 1))\n",
    "xval = np.reshape(xVal.values, (xVal.shape[0], xVal.shape[1], 1))\n",
    "yval = np.reshape(yVal.values, (yVal.shape[0], yVal.shape[1], 1))\n",
    "xtest = np.reshape(xTest.values, (xTest.shape[0], xTest.shape[1], 1))\n",
    "ytest = np.reshape(yTest.values, (yTest.shape[0], yTest.shape[1], 1))\n",
    "\n",
    "# New shapes after third dimension added\n",
    "print(\"Dimensions afther third dimension added:\")\n",
    "print(\"xTrain:\\t\"+str(xtrain.shape))\n",
    "print(\"yTrain:\\t\"+str(ytrain.shape))\n",
    "print(\"xVal:\\t\"+str(xval.shape))\n",
    "print(\"yVal:\\t\"+str(yval.shape))\n",
    "print(\"xTest:\\t\"+str(xtest.shape))\n",
    "print(\"yTest:\\t\"+str(ytest.shape))\n",
    "\n",
    "validation_data = xval.reshape((xval.shape[0], xval.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition:\n",
    "with tf.device('/gpu:0'): \n",
    "    \n",
    "    def build_model(hp): \n",
    "\n",
    "        model = Sequential()\n",
    "        \n",
    "        # model.add(LSTM(hidden_nodes, input_shape=(xtrain.shape[1], 1)))\n",
    "        # First layer: LSTM. Number of units optimization\n",
    "        model.add(LSTM(units = hp.Int('units', \n",
    "                                    min_value = minUnits,\n",
    "                                    max_value = maxUnits, \n",
    "                                    step = stepsUnits,\n",
    "                                    default = defaultUnits),\n",
    "                        return_sequences = True, \n",
    "                        input_shape = (xtrain.shape[1], xtrain.shape[2])))\n",
    "\n",
    "        # Second layer: Dropout with percetange optimization\n",
    "        model.add(\n",
    "            Dropout(hp.Float('dropout', \n",
    "                            min_value = minDropout,\n",
    "                            max_value = maxDropout,\n",
    "                            default = defaultDropout)))\n",
    "        \n",
    "        # Last layer:\n",
    "        model.add(Dense(numPredictions))\n",
    "\n",
    "        print(\"Summary: \")\n",
    "        model.summary()\n",
    "\n",
    "        # # Set Epsilon to 1, in order to fix huge MAPE values.\n",
    "        # keras.backend.set_epsilon(1)\n",
    "\n",
    "        # Model compilation: learning rate optimization\n",
    "        model.compile(loss = loss,\n",
    "                    optimizer = keras.optimizers.Adam(hp.Choice('learning_rate', \n",
    "                                                                values = learningRate)),\n",
    "                    metrics = [keras.metrics.MAE, \n",
    "                            keras.metrics.MAPE, \n",
    "                            keras.metrics.MSE])\n",
    "\n",
    "        return model\n",
    "\n",
    "        # history = model.fit(\n",
    "        #     xtrain.reshape((xtrain.shape[0], xtrain.shape[1], 1)),\n",
    "        #     ytrain,\n",
    "        #     epochs = epchs,\n",
    "        #     batch_size = batch,\n",
    "        #     validation_data = (xval.reshape((xval.shape[0], xval.shape[1], 1)), yval))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training:\n",
    "with tf.device('/gpu:0'): \n",
    "    # Create optimizer using Random Search\n",
    "    lstmOptimizer = RandomSearch(build_model, \n",
    "                                objective = objective,\n",
    "                                max_trials = trials, \n",
    "                                executions_per_trial = executions,\n",
    "                                project_name = projectName,\n",
    "                                overwrite=True)\n",
    "    \n",
    "    # Create optimizer using Bayesian Optimization \n",
    "    # tcnOptimizer = BayesianOptimization(build_model, \n",
    "    #                             objective = objective,\n",
    "    #                             max_trials = trials, \n",
    "    #                             executions_per_trial = executions,\n",
    "    #                             project_name = projectName,\n",
    "    #                             overwrite=True)\n",
    "\n",
    "    # Search space summary\n",
    "    lstmOptimizer.search_space_summary()\n",
    "\n",
    "    # Search execution with epochs\n",
    "    initialTime = time.time()\n",
    "    lstmOptimizer.search(x = xtrain,\n",
    "                        y = ytrain, \n",
    "                        epochs = epchs,\n",
    "                        batch_size = batch,\n",
    "                        validation_data = (xval.reshape((xval.shape[0], xval.shape[1], 1)), yval))\n",
    "    finalTime = time.time()\n",
    "\n",
    "    # Results summary\n",
    "    lstmOptimizer.results_summary()\n",
    "\n",
    "    # Get best model generated\n",
    "    model = lstmOptimizer.get_best_models(num_models = numBestModels)[0]\n",
    "\n",
    "    # Model training with epochs \n",
    "    history = model.fit(\n",
    "                xtrain.reshape((xtrain.shape[0], xtrain.shape[1], 1)),\n",
    "                ytrain,\n",
    "                epochs = epchs,\n",
    "                batch_size = batch,\n",
    "                validation_data = (xval.reshape((xval.shape[0], xval.shape[1], 1)), yval))\n",
    "\n",
    "    # Show model info\n",
    "    print(\"Tiempo de entrenamiento (en segundos):\\t\"+str(finalTime - initialTime))\n",
    "    print(\"Tiempo de entrenamiento (en horas):\\t\"+str((finalTime - initialTime)/3600))\n",
    "    print(history.history.keys())\n",
    "    print(model.history.history)\n",
    "    print(model.history.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation:\n",
    "\n",
    "# Training and Validation loss curves\n",
    "plt.plot(history.history['loss'], label = 'Training loss')\n",
    "plt.plot(history.history['val_loss'], label = 'Validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Model evaluation with validation data\n",
    "score = model.evaluate(xtest.reshape((xtest.shape[0], xtest.shape[1], 1)), ytest)\n",
    "print('Score:', score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
