{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "        print(\"Setting GPU Memory Growth...\")\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        \n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from plot_keras_history import plot_history\n",
    "import sklearn.metrics as metrics\n",
    "import os as os\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import kerastuner as kt\n",
    "from kerastuner.tuners import RandomSearch, BayesianOptimization\n",
    "\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"KT version:\", kt.__version__)\n",
    "print(\"Panda version:\", pd.__version__)\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(\"Physical devices: \", tf.config.list_physical_devices())\n",
    "# Se va a habilitar la dedicacion dinamica de memoria para que la GPU vaya asignando recursos al proceso conforme los vaya necesitando\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "#print(tf.config.list_physical_devices('GPU'))\n",
    "print(\"Build with CUDA: \", tf.test.is_built_with_cuda())\n",
    "\n",
    "# Number of best models\n",
    "numBestModels = 1 \n",
    "\n",
    "# Validation set percentage\n",
    "val_size = 0.3\n",
    "\n",
    "# Batch size\n",
    "batch = 1024\n",
    "\n",
    "# Nodos internos\n",
    "hidden_nodes = 10\n",
    "\n",
    "# Units parameters\n",
    "minUnits = 10\n",
    "maxUnits = 50\n",
    "stepsUnits = 10\n",
    "defaultUnits = 25\n",
    "\n",
    "# Layers parameters\n",
    "minLayers = 1\n",
    "maxLayers = 5\n",
    "defaultLayers = 3\n",
    "\n",
    "# Dropout Parameters\n",
    "minDropout = 0\n",
    "maxDropout = 0.33\n",
    "defaultDropout = .25\n",
    "\n",
    "# Establecer medida de loss\n",
    "loss = \"mean_absolute_error\"\n",
    "\n",
    "# Learning rate\n",
    "learningRate = [0.0, 1e-2, 1e-3, 1e-4]\n",
    "\n",
    "# Model metrics\n",
    "modelMetrics = [keras.metrics.MAE, tf.keras.metrics.RootMeanSquaredError(name = 'rmse')]\n",
    "\n",
    "# Optimizer objetive: error percetange with the validation set \n",
    "objective = kt.Objective('val_mean_absolute_error', 'min')\n",
    "\n",
    "# Maximum model trials and executions\n",
    "trials = 2\n",
    "executions = 5\n",
    "\n",
    "# Model saving parameters\n",
    "workingDirectory = datetime.datetime.fromtimestamp(time.time()).strftime('%d-%m-%Y-%H:%M')\n",
    "# projectName = \"gru\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pd.read_csv(\"C:/Users/hecto/Documents/Master/TFM/tfm-renewable-energy-deep-learning/data/training/2015_PV_W20_H20.csv\",\n",
    "#                        delimiter=\";\")\n",
    "\n",
    "# xTest = pd.read_csv(\"C:/Users/hecto/Documents/Master/TFM/tfm-renewable-energy-deep-learning/data/test/2016_PV_W20_H20.csv\",\n",
    "#                    delimiter=\";\")\n",
    "\n",
    "X = pd.read_csv(\"C:/Users/hecto/Documents/Master/TFM/tfm-renewable-energy-deep-learning/data/training/2015_Weather_W7.csv\",\n",
    "                       delimiter=\",\")\n",
    "\n",
    "xTest = pd.read_csv(\"C:/Users/hecto/Documents/Master/TFM/tfm-renewable-energy-deep-learning/data/test/2016_Weather_W7.csv\",\n",
    "                   delimiter=\",\")\n",
    "\n",
    "print(X)\n",
    "print(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs\n",
    "epchs = 1000\n",
    "\n",
    "W=20\n",
    "H=20\n",
    "PV_index = 14\n",
    "\n",
    "projectNameBO = \"Weather_W7-gru-bo\"\n",
    "projectNameRS = \"Weather_W7-gru-rs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for W type files:\n",
    "\n",
    "# Filter for PV data\n",
    "X = X.iloc[:, ::PV_index]\n",
    "xTest = xTest.iloc[:, ::PV_index]\n",
    "\n",
    "numPredictions = len(X.columns)\n",
    "\n",
    "# Reset column names\n",
    "new_column_names = [f'PV_{i+1}' for i in range(numPredictions)]\n",
    "X.columns = new_column_names\n",
    "xTest.columns = new_column_names\n",
    "\n",
    "Y = pd.DataFrame.copy(X)\n",
    "yTest = pd.DataFrame.copy(xTest)\n",
    "\n",
    "# Create Y from taking next day data\n",
    "Y = Y.drop(0)\n",
    "Y = Y.reset_index(drop = True)\n",
    "Y.loc[len(Y)] = np.zeros(numPredictions)\n",
    "\n",
    "yTest = yTest.drop(0)\n",
    "yTest = yTest.reset_index(drop = True)\n",
    "yTest.loc[len(yTest)] = np.zeros(numPredictions)\n",
    "\n",
    "print(X)\n",
    "print(yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Old data preprocessing: each row will contain the 20 measures for each day , and the 10 measures for the following day\n",
    "# # X = pd.DataFrame(np.array(training).reshape(-1, W))\n",
    "# Y = pd.DataFrame.copy(X)\n",
    "# yTest = pd.DataFrame.copy(xTest)\n",
    "\n",
    "# Y.columns = [\"col_{}\".format(i) for i in range(W+1, (2*W)+1)]\n",
    "# Y = Y.drop(0)\n",
    "# Y = Y.reset_index(drop = True)\n",
    "# Y.loc[len(Y)] = np.zeros(numPredictions)\n",
    "\n",
    "# # Last row is deleted because it is the one used for the real prediction, \n",
    "# # it is not useful for the training of the model. \n",
    "# X.drop(X.tail(1).index, inplace = True)\n",
    "# Y.drop(Y.tail(1).index, inplace = True)\n",
    "\n",
    "# print(\"X Preproccessed shape: \", X.shape)\n",
    "# print(\"Y Preproccessed shape: \", Y.shape)\n",
    "# print(\"---------------------------------------------\")\n",
    "\n",
    "# #  Uncomment in order to normalize data\n",
    "# # Data Normalization\n",
    "# # scaler = MinMaxScaler()\n",
    "# # X = pd.DataFrame(scaler.fit_transform(X), columns=[\"col_{}\".format(i) for i in range(1, 21)])\n",
    "# # Y = pd.DataFrame(scaler.fit_transform(Y), columns=[\"col_{}\".format(i) for i in range(21, 41)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPreproccessed = pd.concat([X, Y], axis = 1)\n",
    "\n",
    "print(\"DataFrame Preproccessed:\")\n",
    "print(dfPreproccessed)\n",
    "print(\"---------------------------------------------\")\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "xTrain, xVal, yTrain, yVal = train_test_split(X, \n",
    "                                            Y, \n",
    "                                            test_size = val_size, \n",
    "                                            random_state = 0, \n",
    "                                            shuffle=False)\n",
    "\n",
    "# Show subsets shapes\n",
    "print(\"Shapes:\")\n",
    "print(\"xTrain:\\t\"+str(xTrain.shape))\n",
    "print(\"yTrain:\\t\"+str(yTrain.shape))\n",
    "print(\"xVal:\\t\"+str(xVal.shape))\n",
    "print(\"yVal:\\t\"+str(yVal.shape))\n",
    "print(\"xTest:\\t\"+str(xTest.shape))\n",
    "print(\"yTest:\\t\"+str(yTest.shape))\n",
    "print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model definition:\n",
    "with tf.device('/gpu:0'): \n",
    "    \n",
    "    def build_model(hp): \n",
    "\n",
    "        model = Sequential()\n",
    "        \n",
    "        # First layer: GRU. Number of units optimization\n",
    "        model.add(GRU(units = hp.Int('units', \n",
    "                                    min_value = minUnits,\n",
    "                                    max_value = maxUnits, \n",
    "                                    step = stepsUnits,\n",
    "                                    default = defaultUnits),\n",
    "                        return_sequences = False, \n",
    "                        input_shape = (xTrain.shape[1], 1)))\n",
    "\n",
    "        # Second layer: Dropout with percetange optimization\n",
    "        model.add(\n",
    "            Dropout(hp.Float('dropout', \n",
    "                            min_value = minDropout,\n",
    "                            max_value = maxDropout,\n",
    "                            default = defaultDropout)))\n",
    "        \n",
    "        # Last layer:\n",
    "        model.add(Dense(numPredictions))\n",
    "\n",
    "        print(\"Summary: \")\n",
    "        model.summary()\n",
    "\n",
    "        # # Set Epsilon to 1, in order to fix huge MAPE values.\n",
    "        # keras.backend.set_epsilon(1)\n",
    "\n",
    "        # Model compilation: learning rate optimization\n",
    "        model.compile(loss = loss,\n",
    "                    optimizer = keras.optimizers.Adam(hp.Choice('learning_rate', \n",
    "                                                                values = learningRate)),\n",
    "                    metrics = modelMetrics)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training:\n",
    "with tf.device('/gpu:0'): \n",
    "    # Create optimizer\n",
    "    gruOptimizer = RandomSearch(build_model, \n",
    "                                objective = objective,\n",
    "                                max_trials = trials, \n",
    "                                executions_per_trial = executions,\n",
    "                                project_name = projectNameRS,\n",
    "                                overwrite=True)\n",
    "    \n",
    "    gruOptimizer.search_space_summary()\n",
    "\n",
    "    # Search execution with epochs\n",
    "    initialTime = time.time()\n",
    "    gruOptimizer.search(x = np.reshape(xTrain.values, (xTrain.shape[0], xTrain.shape[1], 1)),\n",
    "                        y = yTrain, \n",
    "                        epochs = epchs,\n",
    "                        batch_size = batch,\n",
    "                        validation_data=(np.reshape(xVal.values, (xVal.shape[0], xVal.shape[1], 1)), yVal))\n",
    "    finalTime = time.time()\n",
    "\n",
    "    # Results summary\n",
    "    gruOptimizer.results_summary()\n",
    "\n",
    "    # Get best model generated\n",
    "    model = gruOptimizer.get_best_models(num_models = numBestModels)[0]\n",
    "\n",
    "    # Model training with epochs \n",
    "    history = model.fit(\n",
    "                np.reshape(xTrain.values, (xTrain.shape[0], xTrain.shape[1], 1)),\n",
    "                yTrain,\n",
    "                epochs = epchs,\n",
    "                batch_size = batch,\n",
    "                validation_data=(np.reshape(xVal.values, (xVal.shape[0], xVal.shape[1], 1)), yVal))\n",
    "\n",
    "    # Show model info\n",
    "    print(\"Tiempo de entrenamiento (en segundos):\\t\"+str(finalTime - initialTime))\n",
    "    print(\"Tiempo de entrenamiento (en horas):\\t\"+str((finalTime - initialTime)/3600))\n",
    "    print(history.history.keys())\n",
    "    print(model.history.history)\n",
    "    print(model.history.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation loss curves\n",
    "plt.plot(history.history['loss'], label = 'Training loss')\n",
    "plt.plot(history.history['val_loss'], label = 'Validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Model evaluation with validation data\n",
    "score = model.evaluate(np.reshape(xTest.values,(xTest.shape[0], xTest.shape[1], 1)), yTest)\n",
    "print('Score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training:\n",
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    # Create optimizer using Bayesian Optimization \n",
    "    gruOptimizer = BayesianOptimization(build_model, \n",
    "                                objective = objective,\n",
    "                                max_trials = trials, \n",
    "                                executions_per_trial = executions,\n",
    "                                project_name = projectNameBO,\n",
    "                                overwrite=True)\n",
    "    \n",
    "    # Search space summary\n",
    "    gruOptimizer.search_space_summary()\n",
    "\n",
    "    # Search execution with epochs\n",
    "    initialTime = time.time()\n",
    "    gruOptimizer.search(x = np.reshape(xTrain.values, (xTrain.shape[0], xTrain.shape[1], 1)),\n",
    "                        y = yTrain, \n",
    "                        epochs = epchs,\n",
    "                        batch_size = batch,\n",
    "                        validation_data=(np.reshape(xVal.values, (xVal.shape[0], xVal.shape[1], 1)), yVal))\n",
    "    finalTime = time.time()\n",
    "\n",
    "    # Results summary\n",
    "    gruOptimizer.results_summary()\n",
    "\n",
    "    # Get best model generated\n",
    "    model = gruOptimizer.get_best_models(num_models = numBestModels)[0]\n",
    "\n",
    "    # Model training with epochs \n",
    "    history = model.fit(\n",
    "                np.reshape(xTrain.values, (xTrain.shape[0], xTrain.shape[1], 1)),\n",
    "                yTrain,\n",
    "                epochs = epchs,\n",
    "                batch_size = batch,\n",
    "                validation_data=(np.reshape(xVal.values, (xVal.shape[0], xVal.shape[1], 1)), yVal))\n",
    "\n",
    "    # Show model info\n",
    "    print(\"Tiempo de entrenamiento (en segundos):\\t\"+str(finalTime - initialTime))\n",
    "    print(\"Tiempo de entrenamiento (en horas):\\t\"+str((finalTime - initialTime)/3600))\n",
    "    print(history.history.keys())\n",
    "    print(model.history.history)\n",
    "    print(model.history.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation loss curves\n",
    "plt.plot(history.history['loss'], label = 'Training loss')\n",
    "plt.plot(history.history['val_loss'], label = 'Validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Model evaluation with validation data\n",
    "score = model.evaluate(np.reshape(xTest.values,(xTest.shape[0], xTest.shape[1], 1)), yTest)\n",
    "print('Score:', score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
