{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version\n",
    "%pip install --upgrade pandas\n",
    "%pip install openpyxl\n",
    "%pip install numpy\n",
    "%pip install scikit-learn\n",
    "%pip install matplotlib\n",
    "%pip install plot_keras_history\n",
    "%pip install tensorflow\n",
    "%pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from plot_keras_history import plot_history\n",
    "import sklearn.metrics as metrics\n",
    "import os as os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.python.client import device_lib\n",
    "# import keras_tuner as kerastuner\n",
    "# from keras_tuner.tuners import RandomSearch\n",
    "\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Panda version:\", pd.__version__)\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(\"Physical devices: \", tf.config.list_physical_devices())\n",
    "# Se va a habilitar la dedicacion dinamica de memoria para que la GPU vaya asignando recursos al proceso conforme los vaya necesitando\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "#print(tf.config.list_physical_devices('GPU'))\n",
    "print(\"Build with CUDA: \", tf.test.is_built_with_cuda())\n",
    "\n",
    "# Historico de datos para realizar la predicción\n",
    "W = 729\n",
    "# Insantes de tiempo futuros a predecir\n",
    "H = 10\n",
    "\n",
    "# Porcentaje del conjunto de test\n",
    "test_size = 0.3\n",
    "# Porcentaje del conjunto de validacion\n",
    "val_size = 0.3\n",
    "\n",
    "# Establecer objetivo (Name of model metric to minimize or maximize, e.g. \"val_accuracy\"). el \"val_\" hace referencia a que se coge la métrica en el subconjunto de validación\n",
    "\n",
    "# obje = kerastuner.Objective('val_mean_absolute_percentage_error', 'min')\n",
    "\n",
    "# Epocas\n",
    "epchs=1000\n",
    "# Tamaño del batch\n",
    "batch = 1024\n",
    "\n",
    "# Establecer minimo y maximo de capas y el valor por defecto\n",
    "minLayers = 1\n",
    "maxLayers = 10\n",
    "defaultLayers = 3\n",
    "\n",
    "# Establecer learning rate\n",
    "lr = [0.0, 1e-2, 1e-3, 1e-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"C:/Users/hecto/Documents/Master/TFM/tfm-renewable-energy-deep-learning/2015_30min.xlsx\", header=None, engine='openpyxl')[0]\n",
    "\n",
    "# Data preprocessing: each row will contain the 10 measures for each day , and the 10 measures for the following day\n",
    "#X = pd.DataFrame(np.array(df).reshape(-1, 10), columns=[\"col_{}\".format(i) for i in range(0, 10)])\n",
    "X = pd.DataFrame(np.array(df).reshape(-1, 10))\n",
    "Y = pd.DataFrame.copy(X)\n",
    "\n",
    "Y.columns = [\"col_{}\".format(i) for i in range(11, 21)]\n",
    "Y = Y.drop(0)\n",
    "Y = Y.reset_index(drop=True)\n",
    "Y.loc[len(Y)] = np.zeros(10)\n",
    "\n",
    "dfNoNormalized = pd.concat([X, Y], axis=1)\n",
    "\n",
    "dataToPredict = dfNoNormalized.tail(1)\n",
    "print(\"Row for predicting: \", dataToPredict)\n",
    "\n",
    "# Data Normalization\n",
    "scaler = MinMaxScaler()\n",
    "XNormalized = pd.DataFrame(scaler.fit_transform(X), columns=[\"col_{}\".format(i) for i in range(0, 10)])\n",
    "YNormalized = pd.DataFrame(scaler.fit_transform(Y), columns=[\"col_{}\".format(i) for i in range(11, 21)])\n",
    "\n",
    "dfPreproccessed = pd.concat([XNormalized, YNormalized], axis=1)\n",
    "\n",
    "# Last row is deleted beacuse it is the one for used for the real prediction, it is not useful for the training of the model.\n",
    "print(len(dfPreproccessed))\n",
    "dfPreproccessed.drop(dfNoNormalized.tail(1).index, inplace=True)\n",
    "print(len(dfPreproccessed))\n",
    "\n",
    "print(\"DataFrame Preproccessed:\")\n",
    "print(dfPreproccessed)\n",
    "print(\"---------------------------------------------\")\n",
    "\n",
    "pd.set_option('display.max_columns', dfPreproccessed.shape[1])\n",
    "print(\"Head DataFrame Preprocessed\\n \" + str(dfPreproccessed.head()))\n",
    "print(\"---------------------------------------------\")\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(XNormalized, YNormalized, test_size = test_size, random_state = 0, shuffle=False)\n",
    "xTrain, xVal, yTrain, yVal = train_test_split(xTrain, yTrain, test_size = val_size, random_state = 0, shuffle=False)\n",
    "\n",
    "# Show subsets shapes\n",
    "print(\"Shapes:\")\n",
    "print(\"xTrain:\\t\"+str(xTrain.shape))\n",
    "print(\"yTrain:\\t\"+str(yTrain.shape))\n",
    "print(\"xVal:\\t\"+str(xVal.shape))\n",
    "print(\"yVal:\\t\"+str(yVal.shape))\n",
    "print(\"xTest:\\t\"+str(xTest.shape))\n",
    "print(\"yTest:\\t\"+str(yTest.shape))\n",
    "\n",
    "print(\"---------------------------------------------\")\n",
    "\n",
    "# Adding the third dimension needed to use LSTM (samples, timestamps, features)\n",
    "xtrain = np.reshape(xTrain.values, (xTrain.shape[0], xTrain.shape[1], 1))\n",
    "ytrain = np.reshape(yTrain.values, (yTrain.shape[0], yTrain.shape[1], 1))\n",
    "xval = np.reshape(xVal.values, (xVal.shape[0], xVal.shape[1], 1))\n",
    "yval = np.reshape(yVal.values, (yVal.shape[0], yVal.shape[1], 1))\n",
    "xtest = np.reshape(xTest.values, (xTest.shape[0], xTest.shape[1], 1))\n",
    "ytest = np.reshape(yTest.values, (yTest.shape[0], yTest.shape[1], 1))\n",
    "\n",
    "# New shapes after third dimension added\n",
    "print(\"Dimensions afther third dimension added:\")\n",
    "print(\"xTrain:\\t\"+str(xtrain.shape))\n",
    "print(\"yTrain:\\t\"+str(ytrain.shape))\n",
    "print(\"xVal:\\t\"+str(xval.shape))\n",
    "print(\"yVal:\\t\"+str(yval.shape))\n",
    "print(\"xTest:\\t\"+str(xtest.shape))\n",
    "print(\"yTest:\\t\"+str(ytest.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(xtrain.shape[1], 1)))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "print(\"Input shape\", (xtrain.shape[1], 1))\n",
    "\n",
    "print(\"Summary: \")\n",
    "model.summary()\n",
    "\n",
    "# Model compilation\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "            optimizer=\"adam\",\n",
    "            metrics=[\"accuracy\", keras.metrics.MAPE, keras.metrics.MSE])\n",
    "\n",
    "history = model.fit(\n",
    "    xtrain.reshape((xtrain.shape[0], xtrain.shape[1], 1)),\n",
    "    ytrain,\n",
    "    epochs=epchs,\n",
    "    batch_size=16,\n",
    "    validation_data=(xval.reshape((xval.shape[0], xval.shape[1], 1)), yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation loss curves\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Model evaluation with validation data\n",
    "score = model.evaluate(xval.reshape((xval.shape[0], xval.shape[1], 1)), yval)\n",
    "print('Validation loss:', score)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
